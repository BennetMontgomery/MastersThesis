# IMPORTS
import tensorflow as tf
import random
from agent import Agent
from roundabout_gym.roundabout_env import RoundaboutEnv
from collections import namedtuple
from datetime import datetime

# CONSTANTS
SAVE_MODEL = False

'''
    BehaviourNet outputs a behaviour decision from the following:
        - follow leader (car directly in front, or accelerate to speed limit if no front car)
        - choose turn
        - change lane
'''
class BehaviourNet(tf.keras.Model):
    def __init__(self, input_size, layers, memory_cap):
        super(BehaviourNet, self).__init__()
        # Network construction parameters
        self.input_size = input_size
        self.layers = layers

        # Building input layer
        self.input_layer = tf.keras.layers.InputLayer(input_shape=(input_size,))

        # Building fully connected intermediate layers
        self.hidden_layers = [tf.keras.layers.Dense(layer, activation='relu') for layer in layers]

        # Building output layer:
        #   0: change lane left
        #   1: change lane right
        #   2: follow leader
        self.output_layer = tf.keras.layers.Dense(3, activation='linear')

        # Replay buffer parameters
        self.memory_cap = memory_cap

        # instantiate buffer
        self.memory = []
        self.mem_counter = 0

    def add_mem(self, experience):
        # fill to memory cap
        if len(self.memory) < self.memory_cap:
            self.memory.append(experience)
        # or replace a pseudo-random memory if cap reached
        else:
            self.memory[self.counter % self.capacity] = experience

        self.counter += 1

    def sample_replay_batch(self, batch_size):
        if len(self.memory) > batch_size:
            return random.sample(self.memory, batch_size)
        else:
            raise ValueError("[!!] Replay Buffer queried before {batch} memories accumulated".format(batch=batch_size))

    @tf.function
    def __call__(self, inputs, **kwargs):
        input = self.input_layer(inputs)
        for layer in self.hidden_layers:
            input = layer(input)

        output = self.output_layer(input)
        return output


'''
    Brake/Throttle network takes in the current state space and a behaviour code generated by BehaviourNet and 
    converts it to an appropriate brake/speed value
'''
class BTNet(BehaviourNet):
    def __init__(self, state_size, behaviour_code_num, layers, memory_cap):
        super(BTNet, self).__init__(state_size + behaviour_code_num, layers, memory_cap)

        # Building input layer
        self.input_layer = tf.keras.layers.InputLayer(input_shape=(self.memory_size,))

        # Building hidden layers
        self.hidden_layers = [tf.keras.layers.Dense(layer, activation='relu') for layer in layers]

        # Building output layer
        self.output_layer = tf.keras.layers.Dense(22, activation='linear')


class ProposedAgent(Agent):
    def __init__(self, agentid, network, LANEUNITS=0.5, MAX_DECEL=7, MAX_ACCEL=4, verbose=False, VIEW_DISTANCE=30):
        super().__init__(agentid, network, LANEUNITS, MAX_DECEL, MAX_ACCEL, verbose, VIEW_DISTANCE)

        self.behaviour_net = None
        self.throttle_net = None

    def train_nets(self):
        # HYPERPARAMS
        batch_size = 32
        gamma = 0.9
        update_freq = 25
        replay_cap = 8000
        episodes = 2000
        log_freq = 1
        alpha = 0.001
        optimizer = tf.optimizers.Adam(alpha)
        b_layers = [200, 100, 100]
        t_layers = [200, 100, 100]

        # start environment
        test_env = RoundaboutEnv(self)

        # declare replay buffer experience format
        Experience = namedtuple('Experience', ['states', 'actions', 'rewards', 'state_primes', 'terminates'])

        # create policy networks and target networks with equivalent starting parameters
        self.behaviour_net = BehaviourNet()



    def select_action(self, time_step):
        pass